{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import csv\n",
    "import sqlite3 as lite\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import *\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import linear_model, metrics, svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sys.path.append('C:\\Users\\jbadiabo')\n",
    "\n",
    "\n",
    "# get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "\n",
    "# -----------------------------CLASS 'MAKE_PREDICTIONS'-------------------------------\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix'):\n",
    "    plt.title(title)\n",
    "    fig = plt.figure()\n",
    "    # create a grid of subplots\n",
    "    ax = fig.add_subplot(111)\n",
    "    # declare a var that shows the confusion matrix\n",
    "    cax = ax.matshow(cm)\n",
    "    # Create a colorbar for a ScalarMappable instance, mappable\n",
    "    fig.colorbar(cax)\n",
    "    plt.ylabel('True value')\n",
    "    plt.xlabel('Predicted value')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def plot_response(subset_sizes, train_errs, test_errs):\n",
    "    plt.plot(subset_sizes, train_errs, lw=2)\n",
    "    plt.plot(subset_sizes, test_errs, lw=2)\n",
    "    plt.legend(['Training accuracy', 'Test accuracy'])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Dataset size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model response to dataset size')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normalize_features(X):\n",
    "    minmax_scale = preprocessing.MinMaxScaler().fit(X)\n",
    "    X = minmax_scale.transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def standardize_features(X):\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_id_column_to_csv(tableau_input_filename):\n",
    "    df = read_csv(tableau_input_filename)\n",
    "    df.index.name = 'ID'\n",
    "    df.to_csv(tableau_input_filename, mode='w+', index=True)\n",
    "\n",
    "\n",
    "class MLBMakePredictions(object):\n",
    "    # Applies ml techniques to the mlb scraped data and uses the \n",
    "    # resulting model to make predictions on unplayed games \n",
    "    # (NB: unplayed game = mlb_db_name = mlb_team_data_x.db, for ex. the database of the CURRENT SEASON, \n",
    "    # the season you want to make prediction)\n",
    "\n",
    "    from ScrapeMLBTeamStats import AcquireTeamStats\n",
    "    from ScrapeMLBGameStats import AcquireGameStats\n",
    "    from PrepareForMLTechMLB import PrepareForML\n",
    "\n",
    "    def __init__(self, current_season, feature_file, mlb_db_name):\n",
    "        # type: (int, str, str) -> object\n",
    "        \"\"\"\n",
    "        :rtype: object\n",
    "        \"\"\"\n",
    "        self.data = np.load(feature_file)\n",
    "        self.tableau_input_filename = \"mlb_tableau_input\" + str(current_season) + '.csv'\n",
    "        self.current_season = current_season\n",
    "        self.X = self.data['X']\n",
    "        self.y = self.data['y']\n",
    "        self.mlb_db_name = mlb_db_name\n",
    "\n",
    "    def __call__(self):\n",
    "        self.acquire_current_season_data(self.current_season)\n",
    "        # Train again on our data from 'features.npz' because for 'learning curves'\n",
    "        # we performed the training process in the CV function \n",
    "        self.train_logistic_regression()\n",
    "        self.make_tableau_file(self.game_data_filename, self.datetime_filename)\n",
    "        add_id_column_to_csv(self.tableau_input_filename)\n",
    "\n",
    "    # -----------------------ACQUIRE CURRENT SEASON DATA---------------------------\n",
    "    def acquire_current_season_data(self, current_season):\n",
    "        # Acquires all data structures needed to make predictions on current season \n",
    "\n",
    "        team_data_filename = 'mlb_team_stats_' + str(current_season) + '.csv'\n",
    "        game_data_filename = 'mlb_game_stats_' + str(current_season) + '.csv'\n",
    "        datetime_filename = 'mlb_datetime_' + str(current_season) + '.csv'\n",
    "        db_filename = 'mlb_team_data_' + str(current_season) + '.db'\n",
    "        feature_filename = 'mlb_' + str(current_season) + '_features.npz'\n",
    "        # if you want to filter,\n",
    "        # uncomment and make changes to the corresponding section in the 'AcquireGameStats' class\n",
    "\n",
    "        # Scrape data for the current season\n",
    "        mlb_teamdata = self.AcquireTeamStats(current_season, current_season, current_season, team_data_filename)\n",
    "        mlb_teamdata()\n",
    "        mlb_gamedata = self.AcquireGameStats(current_season, current_season, current_season, game_data_filename,\n",
    "                                             datetime_filename)\n",
    "        mlb_gamedata()\n",
    "\n",
    "        # Prepare for ML predictions\n",
    "        pml = self.PrepareForML(game_data_filename, db_filename)\n",
    "        pml.process_raw_data(team_data_filename)\n",
    "        pml(feature_filename)\n",
    "\n",
    "        self.datetime_filename = datetime_filename\n",
    "        self.game_data_filename = game_data_filename\n",
    "\n",
    "    # -------------------------------------------CREATING CSV FILE FOR TABLEAU----------------------------------\n",
    "\n",
    "    def make_tableau_file(self, game_data_filename, datetime_filename):\n",
    "        # Produces a csv file containing predicted and actual game results for the current season\n",
    "        # Tableau uses the contents of the file to produce visualization\n",
    "\n",
    "        with open(self.tableau_input_filename, 'wb') as writefile:\n",
    "            tableau_write = csv.writer(writefile)\n",
    "            tableau_write.writerow(\n",
    "                ['Visitor_Team', 'V_Team_PTS', 'Home_Team', 'H_Team_PTS', 'True_Result', 'Predicted_Result',\n",
    "                 'Confidence', 'Date'])\n",
    "\n",
    "            with open(game_data_filename, 'rb') as readfile, open(datetime_filename, 'rb') as readfile2:\n",
    "                scorereader = csv.reader(readfile)\n",
    "                scores = [row for row in scorereader]\n",
    "                scores = scores[1::]\n",
    "                daysreader = csv.reader(readfile2)\n",
    "                days = [day for day in daysreader]\n",
    "                if (len(scores) != len(days)):\n",
    "                    print(\"File lengths do not match\")\n",
    "                else:\n",
    "                    for i in range(len(days)):\n",
    "                        tableau_content = scores[i][1::]\n",
    "                        tableau_date = days[i]\n",
    "                        # Append True_Result\n",
    "                        try:\n",
    "                            if int(tableau_content[3]) > int(tableau_content[1]):\n",
    "                                tableau_content.append(1)\n",
    "                            else:\n",
    "                                tableau_content.append(0)\n",
    "                        except:\n",
    "                            pass\n",
    "                        # Append 'Predicted_Result' and 'Confidence'\n",
    "                        prediction_results = self.make_predictions(tableau_content[0], tableau_content[2])\n",
    "                        tableau_content += list(prediction_results)\n",
    "                        tableau_content += tableau_date\n",
    "\n",
    "                        tableau_write.writerow(tableau_content)\n",
    "                        # -----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        # -----------------------------ADD INDEX COLUMN TO CSV FOR TABLEAU ANALYSIS-----\n",
    "\n",
    "                        # To run after the 'make_tableau_file' function so as to add 'ID' column to the\n",
    "                        #  'tableau_input' file and so indexing each game...\n",
    "                        # and thus facilitate analysis of each game in Tableau\n",
    "\n",
    "    # E.G. THIS IS THE FILE TO TAKE FOR BETTING STRATEGIES IN TABLEAU!!!!\n",
    "\n",
    "    # ----------------------------------------ALGORITHMS-------------------------------------------------------------------------------------\n",
    "    # Each of the algorithm has a separation between 'instanciating\n",
    "    # + training to the data' and only 'intanciating' (required for k-fold CV)\n",
    "\n",
    "    # Logistic Regression\n",
    "    def train_logistic_regression(self, scale_data=False):\n",
    "        # IF YOU PASS 'scale_data' to True\n",
    "        # DO NOT FORGET TO DO THE SAME IN 'make_predictions' to also normalize test data (current_season features)\n",
    "        # Preprocessing step: False by default\n",
    "        # Scaling the feature vector applying normalization 'Min-Max scaling' technique\n",
    "        # If you want to use standardization use 'standardize_features' function\n",
    "        # Hint: Better use normalization for SVM\n",
    "        # (http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) or try both and\n",
    "        # compare the results with cross validation\n",
    "        if scale_data != False:\n",
    "            self.X = standardize_features(self.X)\n",
    "        else:\n",
    "            pass\n",
    "        X, y = shuffle(self.X, self.y)\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "        self.logreg.fit(X, y)\n",
    "\n",
    "    def instantiate_logistic_regression(self):\n",
    "        # Only instantiate logistic regression model without fitting any data\n",
    "        # Needed in the 'model_evaluation' function\n",
    "        self.logreg2 = linear_model.LogisticRegression()\n",
    "        pass\n",
    "\n",
    "    # Radial Basis Function kernel SVM\n",
    "    def train_rbf_svm(self, scale_data=True):\n",
    "        # IF YOU PASS 'scale_data' to True DO NOT FORGET TO DO THE SAME IN 'make_predictions' \n",
    "        # to also normalize test data (current_season features)        \n",
    "        # Preprocessing step: True by default(only for SVM as it is always recommended)\n",
    "        # Scaling the feature vector applying normalization 'Min-Max scaling' technique\n",
    "        # If you want to use standardization use 'standardize_features' function\n",
    "        # Hint: Better use normalization for SVM\n",
    "        # (http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) or try both and\n",
    "        # compare the results with cross validation\n",
    "        if scale_data != False:\n",
    "            self.X = normalize_features(self.X)\n",
    "        else:\n",
    "            pass\n",
    "        X, y = shuffle(self.X, self.y)\n",
    "        self.clf = svm.SVC(probability=True, random_state=None)\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def instantiate_rbf_svm(self):\n",
    "        self.clf2 = svm.SVC(probability=True, random_state=None)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def train_adaboost(self):\n",
    "        X, y = shuffle(self.X, self.y)\n",
    "        self.dbt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
    "        self.dbt.fix(X, y)\n",
    "\n",
    "    def instantiate_adaboost(self):\n",
    "        self.dbt2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
    "        pass\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # -----------------------MAKE PREDICTIONS (implemented in the 'make_tableau_file function)-----------------\n",
    "\n",
    "    def make_predictions(self, team1, team2, scale_data=False):\n",
    "        # Using prediction model, returns 1 if the model thinks team2 will beat team1, 0 otherwise\n",
    "        # Advise: Respect the order:\n",
    "        # V_Team for team1, H_Team for team2 for consistency with the PrepareForML class techniques\n",
    "\n",
    "        query = 'SELECT * FROM Team_Stats WHERE Team = ?'\n",
    "\n",
    "        con = lite.connect(self.mlb_db_name)\n",
    "        with con:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(query, (team1,))\n",
    "            feature1 = list(cur.fetchone()[2::])\n",
    "            cur.execute(query, (team2,))\n",
    "            feature2 = list(cur.fetchone()[2::])\n",
    "            feature = np.array(feature2).reshape(1, -1) - np.array(feature1).reshape(1, -1)\n",
    "\n",
    "            if scale_data != False:\n",
    "                feature = normalize_features(feature)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Make prediction \n",
    "            # TO CHANGE ACCORDING THE ALGORITHM YOU WANT TO USE, \n",
    "            # available classifiers: logreg, clf, dbt (change 2X)\n",
    "            prediction_output = self.logreg.predict(feature)  # Predict class labels for samples in X\n",
    "            prediction_probability = max(\n",
    "                self.logreg.predict_proba(feature)[0])  # Returns the probability of \"prediction_output\"\n",
    "\n",
    "            return prediction_output[0], prediction_probability\n",
    "\n",
    "    # -------------------------------------PARAMETRIC--------------------------------------------\n",
    "\n",
    "    def cval_score(self):\n",
    "        # change 'logreg' to 'clf' if you want to perform it on SVM (dbt also available)\n",
    "        scores = cross_val_score(self.logreg, self.X, self.y, cv=10)\n",
    "        print scores.mean(), scores.std()\n",
    "\n",
    "    # Used in for learning curves and model evaluation\n",
    "    def train_test_split(self):\n",
    "        self.trX, self.teX, self.trY, self.teY = train_test_split(self.X, self.y, test_size=0.30, random_state=None)\n",
    "        pass\n",
    "\n",
    "    # ----------------------------------CROSS VALIDATION AND MODEL EVALUATION-----------------------------\n",
    "\n",
    "    # The below function takes a model,\n",
    "    # a pre-split dataset(train/test X and Y arrays (Nb: use train_test_split function to do so)),\n",
    "    # a scoring function as input and iterates through the dataset training\n",
    "    # on n exponentially spaced subsets and returns the learning curves\n",
    "    # e.g. score_func = metrics.accuracy_score\n",
    "    def data_size_response(self, model, score_func, prob=True, n_subsets=10):\n",
    "\n",
    "        # creating 2 empty arrays for train\n",
    "        train_errs, test_errs = [], []\n",
    "        # defining a var that take the value of \"n_subsets\" and creates \"n_subsets\" with corresponding size\n",
    "        # \"linspace returns num evenly spaced samples, calculated over the interval [start, stop]\n",
    "        # shape[0] of an array Y of dimensions (n,m) merely means \"n\", number of rows\n",
    "        subset_sizes = np.exp(np.linspace(3, np.log(self.trX.shape[0]), n_subsets)).astype(int)\n",
    "\n",
    "        # looping over the subset_sizes\n",
    "        for m in subset_sizes:\n",
    "            # for each subset we fit the model on trX, trY\n",
    "            model.fit(self.trX[:m], self.trY[:m])\n",
    "            # if prob is 'True', we defining var 'train_err' & 'test_err'\n",
    "            # which are scores of Y (true Y) and 'predict_proba' made on X (predict Y)\n",
    "            # respectively for the train and test set\n",
    "            if prob:\n",
    "                train_err = score_func(self.trY[:m], model.predict_proba(self.trX[:m]))\n",
    "                test_err = score_func(self.teY, model.predict_proba(self.teX))\n",
    "            # if prob is 'True', we defining var 'train_err' & 'test_err'\n",
    "            # which are scores of Y (true Y) and 'predict' made on X (predict Y)\n",
    "            # respectively for the train and test set\n",
    "            else:\n",
    "                train_err = score_func(self.trY[:m], model.predict(self.trX[:m]))\n",
    "                test_err = score_func(self.teY, model.predict(self.teX))\n",
    "            # printing the results for the m subset\n",
    "            print \"training error(accuracy): %.3f test error(accuracy): %.3f subset size: %.3f\" % (\n",
    "                train_err, test_err, m)\n",
    "            # appending the m results to the arrays 'train_errs' & 'test_errs'\n",
    "            train_errs.append(train_err)\n",
    "            test_errs.append(test_err)\n",
    "\n",
    "        # returning the number of subsets and the arrays\n",
    "        return subset_sizes, train_errs, test_errs\n",
    "\n",
    "    # Plotting function for visualizing the above response, e.g. the train error and the test error\n",
    "\n",
    "    # Instruction to execute the above functions\n",
    "    # model = it is the model we use e.g LogisticRegression()\n",
    "    # score_func = it is the score function we use e.g. cval_score()\n",
    "    # We declare a variable response = data_size_response(model, trX, teX, trY, teY, score_func, prob=True, n_subsets=?)\n",
    "    # We plot the response: plot_response(*response)\n",
    "\n",
    "    def model_evaluation(self):\n",
    "\n",
    "        # Do not forget to replace the classifier by the one you want to evaluate\n",
    "        # Classifiers: logreg2, clf2, dbt2\n",
    "        self.train_test_split()\n",
    "        y_pred = self.logreg2.fit(self.trX, self.trY).predict(self.teX)\n",
    "        # predictions are in columns and actual values in rows\n",
    "        cm = metrics.confusion_matrix(self.teY, y_pred)\n",
    "        # nb: The support is the number of occurrences of each class in y_true (e.g. what really happened)\n",
    "        print metrics.classification_report(self.teY, y_pred)\n",
    "        print \"matthews correlation coefficent: %.2f\" % (metrics.matthews_corrcoef(self.teY, y_pred))\n",
    "        accuracy = float(cm[0][0] + cm[1][1]) / cm.sum()\n",
    "        print \"accuracy: %.2f\" % (accuracy)\n",
    "        away_accuracy = float(cm[0][0]) / (cm[0][0] + cm[0][1])\n",
    "        print \"away_accuracy: %.2f\" % (away_accuracy)\n",
    "        home_accuracy = float(cm[1][1]) / (cm[1][1] + cm[1][0])\n",
    "        print \"home_accuracy: %.2f\" % (home_accuracy)\n",
    "        plot_confusion_matrix(cm, title='Confusion matrix')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}