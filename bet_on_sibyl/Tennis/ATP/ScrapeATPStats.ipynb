{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import unicodedata\n",
    "from multiprocessing import Process\n",
    "import datetime\n",
    "\n",
    "    \n",
    "class ScrapeATPStats(object):\n",
    "    \n",
    "    from AcquireBetBrainATPUpcomingGames import AcquireBetBrainATPUpcomingGames\n",
    "    \n",
    "    def __init__(self, split_list, for_current_season):\n",
    "        self.split_list = split_list\n",
    "        self.players_game_data = []\n",
    "        self.players_stat_data = []\n",
    "        self.for_current_season = for_current_season\n",
    "        self.current_datetime = datetime.datetime.now()\n",
    "        self.beginning_month_to_scrape_current_season_stats = 3 # We will start scraping current season stats for predictions from March\n",
    "        \n",
    "    def __call__(self):\n",
    "        print \"Scraping data from \" + str(self.split_list[0]) + \" to \" + str(self.split_list[-1])\n",
    "        for self.i in self.split_list:\n",
    "            self.go_on_atp_ranking_page()\n",
    "            self.go_on_player_profile_page(self.browser)\n",
    "            self.go_on_player_statistics_page(self.browser)\n",
    "            self.scrape_player_stat_data(self.browser, self.players_stat_data,\\\n",
    "                                         self.for_current_season, self.current_datetime, self.beginning_month_to_scrape_current_season_stats)\n",
    "            self.go_on_player_game_data_page(self.browser, self.for_current_season)\n",
    "            self.scrape_player_game_data(self.browser, self.players_game_data, self.for_current_season)\n",
    "            print \"---Next player---\"\n",
    "\n",
    "            self.data_to_csv(self.players_stat_data, self.players_game_data, self.split_list)\n",
    "            \n",
    "        if self.for_current_season == \"Yes\":\n",
    "            print \"---Scraping upco games from betbrain for merging---\"\n",
    "            j = self.AcquireBetBrainATPUpcomingGames()\n",
    "            j()\n",
    "            \n",
    "            print \"---Done---\\n\"\n",
    "            \n",
    "        else:\n",
    "            print \"---Done---\\n\"\n",
    "            \n",
    "            \n",
    "    # --- Going on the core tennis website ---\n",
    "    def go_on_atp_ranking_page(self):\n",
    "        url = \"http://www.coretennis.net/\"\n",
    "        delay = 15 # seconds\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                browser = webdriver.Chrome(\"C:\\Users\\jbadiabo\\PycharmProjects\\Sibyl\\chromedriver.exe\")\n",
    "                browser.set_page_load_timeout(delay)\n",
    "                browser.get(url)\n",
    "                # ---Going on the ATP Ranking page---\n",
    "                browser.maximize_window()\n",
    "                \n",
    "                try: \n",
    "                    atp_ranking_button = browser.find_element_by_xpath(\".//div[@class='menu1']/a[4]\")\n",
    "                    atp_ranking_url = atp_ranking_button.get_attribute(\"href\")\n",
    "                except NoSuchElementException:\n",
    "                    atp_ranking_button = browser.find_element_by_partial_link_text(\"Tennis Rankings\")\n",
    "                    atp_ranking_url = atp_ranking_button.get_attribute(\"href\")\n",
    "                    \n",
    "            except TimeoutException:\n",
    "                print \"Timeout going on atp ranking page, retrying\"\n",
    "                browser.quit()\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        try:\n",
    "            atp_ranking_button.click()\n",
    "            browser.get(browser.current_url)\n",
    "        except TimeoutException:\n",
    "            while True:\n",
    "                try:\n",
    "                    print \"Timeout opening atp ranking page, keeping the page anyway..\"\n",
    "                    print \"opening: \" + atp_ranking_url\n",
    "                    browser.get(atp_ranking_url)\n",
    "                except TimeoutException:\n",
    "                    print \"Timeout getting atp ranking page retrying..\"\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        self.browser = browser\n",
    "        \n",
    "    def go_on_player_profile_page(self, browser):\n",
    "        # ---Going on player profile---\n",
    "        current_url = browser.current_url\n",
    "        delay = 15\n",
    "        \n",
    "        try: \n",
    "            try:\n",
    "                table = browser.find_element_by_id('rtable2')\n",
    "            except NoSuchElementException:\n",
    "                table = browser.find_elements_by_tag_name(\"table\")[0]\n",
    "\n",
    "            body = table.find_element_by_tag_name('tbody')\n",
    "            table_rows = body.find_elements_by_tag_name('tr')\n",
    "            table_rows = table_rows[1:]\n",
    "            player_name_cell = table_rows[self.i].find_elements_by_tag_name('td')[1]\n",
    "            player_profile_link = player_name_cell.find_element_by_tag_name('a')    \n",
    "            player_profile_url = player_profile_link.get_attribute('href')\n",
    "            player_profile_link.click()\n",
    "            browser.get(browser.current_url)\n",
    "        except TimeoutException:\n",
    "            while True:\n",
    "                try:        \n",
    "                    print \"Timeout opening player page, retrying, should open player profile page..\"\n",
    "                    browser.quit()\n",
    "                    browser = webdriver.Chrome(\"C:\\Users\\jbadiabo\\PycharmProjects\\Sibyl\\chromedriver.exe\")\n",
    "                    delay += 10\n",
    "                    browser.set_page_load_timeout(delay)\n",
    "                    print \"opening: \" + player_profile_url\n",
    "                    browser.get(player_profile_url)\n",
    "                    browser.maximize_window()\n",
    "                except TimeoutException:\n",
    "                    print \"Timeout on recreeating the driver, retrying..\"\n",
    "                    continue\n",
    "                else:\n",
    "                    break  \n",
    "\n",
    "        self.browser = browser\n",
    "\n",
    "    # ---Going on player statistics profile---\n",
    "    def go_on_player_statistics_page(self, browser):\n",
    "        current_url = browser.current_url\n",
    "        delay = 15\n",
    "        \n",
    "        try:\n",
    "            try:\n",
    "                player_statistics_link = browser.find_element_by_link_text(\"Statistics\")\n",
    "                player_statistics_url = player_statistics_link.get_attribute('href')\n",
    "            except NoSuchElementException:\n",
    "                player_statistics_link = browser.find_element_by_xpath(\".//div[@class='ppNav']/a[4]\")\n",
    "                player_statistics_url = player_statistics_link.get_attribute('href')\n",
    "\n",
    "            player_statistics_link.click()\n",
    "            browser.get(browser.current_url)\n",
    "        except TimeoutException:\n",
    "            while True:\n",
    "                try:        \n",
    "                    print \"Timeout going on stats page, retrying, should open player stat profile..\"\n",
    "                    browser.quit()\n",
    "                    browser = webdriver.Chrome(\"C:\\Users\\jbadiabo\\PycharmProjects\\Sibyl\\chromedriver.exe\")\n",
    "                    delay += 10\n",
    "                    browser.set_page_load_timeout(delay)\n",
    "                    print \"opening: \" + player_statistics_url\n",
    "                    browser.get(player_statistics_url)\n",
    "                    browser.maximize_window()\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "                else:\n",
    "                    break         \n",
    "\n",
    "        self.browser = browser\n",
    "        \n",
    "    # ---Scraping player stats---\n",
    "    def scrape_player_stat_data(self, browser, players_stat_data, for_current_season, current_datetime, beginning_month_to_scrape_current_season_stats):\n",
    "        main_div = browser.find_element_by_id(\"colMainContent1b\")\n",
    "\n",
    "        player_name_header = main_div.find_element_by_class_name(\"ppHeader\")\n",
    "        player_name = player_name_header.find_element_by_tag_name('h1').text.encode(\"ascii\", \"ignore\")\n",
    "\n",
    "        player_name = player_name.rsplit(' ', 2)[0] # 2 for 2nd split starting from the right\n",
    "\n",
    "        table_titles = main_div.find_elements_by_tag_name(\"h2\")\n",
    "        table_titles = [x.text.encode('ascii', 'ignore') for x in table_titles]\n",
    "        stat_tables = main_div.find_elements_by_class_name(\"sTable\")\n",
    "        number_of_years = len(stat_tables)\n",
    "\n",
    "        year_player_data = []\n",
    "        \n",
    "        if for_current_season == 'No': # Mostly used for training, we take all season stat data\n",
    "\n",
    "            for stat_table, table_title in zip(stat_tables, table_titles):\n",
    "\n",
    "                try:\n",
    "                    year = table_title.split(\" \")[0]\n",
    "                    # ------------------------------------------------------------------------------------\n",
    "                    table_footer = stat_table.find_element_by_tag_name(\"tfoot\")\n",
    "                    table_footer_row = table_footer.find_element_by_tag_name(\"tr\")\n",
    "                    data = [player_name, year]\n",
    "                    table_footer_row_data = table_footer_row.find_elements_by_tag_name(\"td\")\n",
    "\n",
    "                    for i in table_footer_row_data:\n",
    "                        data.append(i.text.encode('ascii', 'ignore'))\n",
    "\n",
    "                    year_player_data.append(data)\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "                    \n",
    "        else: # Meaning for_current_season == 'Yes'\n",
    "              # Then we have 2 cases:\n",
    "            \n",
    "            if current_datetime.month >= beginning_month_to_scrape_current_season_stats: # Post March: Then we scrape current season data for predictions\n",
    "            \n",
    "                for stat_table, table_title in zip(stat_tables, table_titles):\n",
    "\n",
    "                    try:\n",
    "                        year = table_title.split(\" \")[0]\n",
    "                        if year == str(current_datetime.year):\n",
    "                            # ------------------------------------------------------------------------------------\n",
    "                            table_footer = stat_table.find_element_by_tag_name(\"tfoot\")\n",
    "                            table_footer_row = table_footer.find_element_by_tag_name(\"tr\")\n",
    "                            data = [player_name, year]\n",
    "                            table_footer_row_data = table_footer_row.find_elements_by_tag_name(\"td\")\n",
    "\n",
    "                            for i in table_footer_row_data:\n",
    "                                data.append(i.text.encode('ascii', 'ignore'))\n",
    "\n",
    "                            year_player_data.append(data)\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    except NoSuchElementException:\n",
    "                        continue\n",
    "                        \n",
    "            else: # Ante March: Then we scrape data of the previous season (ex: 2016) for the current_season_predictions\n",
    "                \n",
    "                for stat_table, table_title in zip(stat_tables, table_titles):\n",
    "\n",
    "                    try:\n",
    "                        year = table_title.split(\" \")[0]\n",
    "                        if year == str(current_datetime.year - 1):\n",
    "                            # ------------------------------------------------------------------------------------\n",
    "                            table_footer = stat_table.find_element_by_tag_name(\"tfoot\")\n",
    "                            table_footer_row = table_footer.find_element_by_tag_name(\"tr\")\n",
    "                            data = [player_name, year]\n",
    "                            table_footer_row_data = table_footer_row.find_elements_by_tag_name(\"td\")\n",
    "\n",
    "                            for i in table_footer_row_data:\n",
    "                                data.append(i.text.encode('ascii', 'ignore'))\n",
    "\n",
    "                            year_player_data.append(data)\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    except NoSuchElementException:\n",
    "                        continue                \n",
    "                    \n",
    "                    \n",
    "        players_stat_data += year_player_data\n",
    "        \n",
    "        self.players_stat_data = players_stat_data\n",
    "\n",
    "        self.browser = browser\n",
    "        \n",
    "    # ---Going on the player game data---\n",
    "    def go_on_player_game_data_page(self, browser):\n",
    "        delay = 15\n",
    "        try:\n",
    "            main_div = browser.find_element_by_id(\"colMainContent1b\")\n",
    "            nav_div = main_div.find_element_by_class_name(\"ppNav\")\n",
    "\n",
    "            try:\n",
    "                results_button = nav_div.find_element_by_partial_link_text(\"Results\")\n",
    "                results_url = results_button.get_attribute(\"href\")\n",
    "            except NoSuchElementException:\n",
    "                results_button = nav_div.find_element_by_xpath(\".//div[@class='ppNav']/a[3]\")\n",
    "                results_url = results_button.get_attribute(\"href\")\n",
    "\n",
    "            results_button.click()\n",
    "            browser.get(browser.current_url)\n",
    "        except TimeoutException:\n",
    "            while True:\n",
    "                try:        \n",
    "                    print \"Timeout going on game stat page, retrying, should open player game stat data page..\"\n",
    "                    browser.quit()\n",
    "                    browser = webdriver.Chrome(\"C:\\Users\\jbadiabo\\PycharmProjects\\Sibyl\\chromedriver.exe\")\n",
    "                    delay += 10\n",
    "                    browser.set_page_load_timeout(delay)\n",
    "                    print \"opening: \" + results_url\n",
    "                    browser.get(results_url)\n",
    "                    browser.maximize_window()\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "                else:\n",
    "                    break     \n",
    "\n",
    "        self.browser = browser\n",
    "\n",
    "    # ---Scraping player game_data---\n",
    "    def scrape_player_game_data(self, browser, players_game_data, for_current_season):\n",
    "        player_game_data = []\n",
    "\n",
    "        year_nav_div = browser.find_element_by_class_name(\"shadetabs\")\n",
    "        year_button_links = year_nav_div.find_elements_by_tag_name(\"li\")\n",
    "\n",
    "        main_div = browser.find_element_by_class_name(\"tabcontentstyle\")\n",
    "        year_divs = main_div.find_elements_by_class_name(\"tabcontent\")\n",
    "        \n",
    "        if for_current_season == 'No': # Mostly used for training, we take all season stat data\n",
    "\n",
    "            for year_button_link, year_div in zip(year_button_links, year_divs):\n",
    "                year_button_link.click()\n",
    "\n",
    "                # --- Scraping the year ---\n",
    "                year = year_nav_div.find_element_by_css_selector(\"li.selected\").text.encode(\"ascii\", \"ignore\")\n",
    "\n",
    "                # ---Scraping player name---\n",
    "                header = browser.find_element_by_class_name(\"ppHeader\")\n",
    "                player_name = header.find_element_by_css_selector('h1').text.encode(\"ascii\", \"ignore\")\n",
    "                player_name = player_name.rsplit(' ', 2)[0] # 2 for 2nd split starting from the right\n",
    "\n",
    "                # ---Scraping game data---\n",
    "                containers = year_div.find_elements_by_class_name(\"pprContainer\")\n",
    "                year_container_rows_data = []\n",
    "\n",
    "                for container in containers:\n",
    "\n",
    "                    container_datetime_range = container.find_element_by_css_selector(\"div.pprHead\")\n",
    "                    try:\n",
    "                        container_datetime_range = container_datetime_range.find_elements_by_tag_name(\"div\")[0].text.encode(\"ascii\", \"ignore\")\n",
    "                    except NoSuchElementException:\n",
    "                        container_datetime_range = container_datetime_range.find_element_by_class_name(\"plM1\")\n",
    "\n",
    "                    container_datetime_range = container_datetime_range.replace(\"\\n\", \" - \")\n",
    "\n",
    "                    container_rows = container.find_elements_by_css_selector(\"div.pprRow\")\n",
    "\n",
    "                    container_rows_data = []\n",
    "                    for row in container_rows:\n",
    "                        data = row.find_elements_by_tag_name(\"div\")\n",
    "                        data = [x.text.encode(\"ascii\", \"ignore\") for x in data]\n",
    "\n",
    "                        indices = 0, 2, 4\n",
    "                        data = [i for j, i in enumerate(data) if j not in indices]\n",
    "                        data.insert(0, data.pop(1))\n",
    "                        data.insert(0, player_name)\n",
    "                        data.insert(0, container_datetime_range)\n",
    "                        data.insert(0, year)\n",
    "\n",
    "\n",
    "                        container_rows_data.append(data)\n",
    "\n",
    "                    year_container_rows_data = year_container_rows_data + container_rows_data\n",
    "\n",
    "                player_game_data = player_game_data + year_container_rows_data\n",
    "                \n",
    "        else: # Meaning for_current_season == 'Yes'\n",
    "            \n",
    "            for year_button_link, year_div in zip(year_button_links[0], year_divs[0]): # Meaning we only scrape the top year == current year\n",
    "                year_button_link.click()\n",
    "\n",
    "                # --- Scraping the year ---\n",
    "                year = year_nav_div.find_element_by_css_selector(\"li.selected\").text.encode(\"ascii\", \"ignore\")\n",
    "\n",
    "                # ---Scraping player name---\n",
    "                header = browser.find_element_by_class_name(\"ppHeader\")\n",
    "                player_name = header.find_element_by_css_selector('h1').text.encode(\"ascii\", \"ignore\")\n",
    "                player_name = player_name.rsplit(' ', 2)[0] # 2 for 2nd split starting from the right\n",
    "\n",
    "                # ---Scraping game data---\n",
    "                containers = year_div.find_elements_by_class_name(\"pprContainer\")\n",
    "                year_container_rows_data = []\n",
    "\n",
    "                for container in containers:\n",
    "\n",
    "                    container_datetime_range = container.find_element_by_css_selector(\"div.pprHead\")\n",
    "                    try:\n",
    "                        container_datetime_range = container_datetime_range.find_elements_by_tag_name(\"div\")[0].text.encode(\"ascii\", \"ignore\")\n",
    "                    except NoSuchElementException:\n",
    "                        container_datetime_range = container_datetime_range.find_element_by_class_name(\"plM1\")\n",
    "\n",
    "                    container_datetime_range = container_datetime_range.replace(\"\\n\", \" - \")\n",
    "\n",
    "                    container_rows = container.find_elements_by_css_selector(\"div.pprRow\")\n",
    "\n",
    "                    container_rows_data = []\n",
    "                    for row in container_rows:\n",
    "                        data = row.find_elements_by_tag_name(\"div\")\n",
    "                        data = [x.text.encode(\"ascii\", \"ignore\") for x in data]\n",
    "\n",
    "                        indices = 0, 2, 4\n",
    "                        data = [i for j, i in enumerate(data) if j not in indices]\n",
    "                        data.insert(0, data.pop(1))\n",
    "                        data.insert(0, player_name)\n",
    "                        data.insert(0, container_datetime_range)\n",
    "                        data.insert(0, year)\n",
    "\n",
    "\n",
    "                        container_rows_data.append(data)\n",
    "\n",
    "                    year_container_rows_data = year_container_rows_data + container_rows_data\n",
    "\n",
    "                player_game_data = player_game_data + year_container_rows_data                \n",
    "            \n",
    "            \n",
    "        players_game_data = players_game_data + player_game_data\n",
    "        \n",
    "        self.players_game_data = players_game_data\n",
    "\n",
    "        browser.quit()\n",
    "        \n",
    "    def data_to_csv(self, players_stat_data, players_game_data, split_list, for_current_season):\n",
    "        \n",
    "        players_stat_data_df = pd.DataFrame(players_stat_data, \n",
    "                                            columns=['Player_name', 'Year', 'Tourn', 'Titles', 'Matches', 'Wins', 'Losses', 'PCT', '6-0', '0-6', '7-6', '6-7'])\n",
    "        players_stat_data_df['Player_name'] = players_stat_data_df['Player_name'].str.rstrip().str.lstrip().str.replace('-', ' ')\n",
    "        players_stat_data_df = players_stat_data_df[players_stat_data_df.Year != \"2017\"]\n",
    "        players_stat_data_df = players_stat_data_df.sort_values(['Year'])\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "\n",
    "        players_game_data_df = pd.DataFrame(players_game_data, \n",
    "                                            columns=['Year', 'Week', 'Player_A', 'Player_B', 'True_Result'])\n",
    "        players_game_data_df['Player_B'] = players_game_data_df['Player_B'].str[:-5]\n",
    "        players_game_data_df.insert(1, 'Start', players_game_data_df['Week'].str.split(' - ').str.get(0))\n",
    "        players_game_data_df.drop('Week', axis = 1, inplace = True)\n",
    "        players_game_data_df.insert(1, 'Week', players_game_data_df['Year'] + ',' + players_game_data_df['Start'])\n",
    "        players_game_data_df.drop('Start', axis = 1, inplace = True)\n",
    "        players_game_data_df['Week'] = players_game_data_df['Week'].str.replace(' ', ',')\n",
    "        players_game_data_df['Week'] = pd.to_datetime(players_game_data_df['Week'], infer_datetime_format=True)\n",
    "        players_game_data_df['Player_A'] = players_game_data_df['Player_A'].str.rstrip().str.lstrip().str.replace('-', ' ')\n",
    "        players_game_data_df['Player_B'] = players_game_data_df['Player_B'].str.rstrip().str.lstrip().str.replace('-', ' ')\n",
    "\n",
    "        # Dropping duplicate rows\n",
    "        players_game_data_df['Winner'] = players_game_data_df.apply(lambda x: x['Player_A'] if x['True_Result'] == 'W' else x['Player_B'], axis = 1)\n",
    "        players_game_data_df['Looser'] = players_game_data_df.apply(lambda x: x['Player_A'] if x['True_Result'] == 'L' else x['Player_B'], axis = 1)\n",
    "        players_game_data_df.drop_duplicates(subset=['Week', 'Winner', 'Looser'], inplace=True)\n",
    "        players_game_data_df.drop(['Winner', 'Looser'], axis = 1, inplace = True)\n",
    "        \n",
    "        # -----------------------------------------------------------------------\n",
    "        players_game_data_df = players_game_data_df[players_game_data_df.Year != \"2017\"]\n",
    "        players_game_data_df = players_game_data_df.sort_values(['Year', 'Week'])\n",
    "\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        if for_current_season == \"No\":\n",
    "            players_stat_data_df.to_csv(\"atp_player_stats_until_\" + str(split_list[-1]) + \"_.csv\", mode='w+')\n",
    "\n",
    "            players_game_data_df['Week'].to_csv(\"date_atp_game_stats.csv\", mode='w+', header=False, index=False)\n",
    "            players_game_data_df.drop('Week', axis = 1, inplace = True)\n",
    "            players_game_data_df.to_csv(\"atp_game_stats_until_\" + str(split_list[-1]) + \"_.csv\", index=False, mode='w+')\n",
    "            \n",
    "        else: # for the current season\n",
    "            players_stat_data_df.to_csv(\"atp_player_stats_until_\" + str(split_list[-1]) + \"_current_season.csv\", mode='w+')\n",
    "\n",
    "            players_game_data_df['Week'].to_csv(\"date_atp_game_stats_current_season.csv\", mode='w+', header=False, index=False)\n",
    "            players_game_data_df.drop('Week', axis = 1, inplace = True)\n",
    "            players_game_data_df.to_csv(\"atp_game_stats_until_\" + str(split_list[-1]) + \"_current_season.csv\", index=False, mode='w+')\n",
    "            \n",
    "        self.players_stat_data_df = players_stat_data_df\n",
    "        self.players_game_data_df = players_game_data_df\n",
    "        \n",
    "\n",
    "def scraping_split_one():\n",
    "    a = ScrapeATPStats(range(1), \"Yes\")\n",
    "    a()\n",
    "\n",
    "def scraping_split_two():\n",
    "    b = ScrapeATPStats(range(11, 21), \"Yes\")\n",
    "    b()\n",
    "    \n",
    "def scraping_split_three():\n",
    "    c = ScrapeATPStats(range(21, 31), \"Yes\")\n",
    "    c()\n",
    "\n",
    "def scraping_split_four():\n",
    "    d = ScrapeATPStats(range(31, 41), \"Yes\")\n",
    "    d()    \n",
    "    \n",
    "def scraping_split_five():\n",
    "    e = ScrapeATPStats(range(41, 51), \"Yes\")\n",
    "    e()\n",
    "\n",
    "def scraping_split_six():\n",
    "    f = ScrapeATPStats(range(51, 61), \"Yes\")\n",
    "    f()\n",
    "    \n",
    "def scraping_split_seven():\n",
    "    g = ScrapeATPStats(range(61, 71), \"Yes\")\n",
    "    g()\n",
    "\n",
    "def scraping_split_eight():\n",
    "    h = ScrapeATPStats(range(71, 81), \"Yes\")\n",
    "    h()  \n",
    "\n",
    "def scraping_split_nine():\n",
    "    i = ScrapeATPStats(range(81, 91), \"Yes\")\n",
    "    i()\n",
    "\n",
    "def scraping_split_ten():\n",
    "    j = ScrapeATPStats(range(91, 101), \"Yes\")\n",
    "    j() \n",
    "    \n",
    "def run_in_parallel(*fns):\n",
    "    proc = []\n",
    "    for fn in fns:\n",
    "        p = Process(target = fn)\n",
    "        p.start()\n",
    "        proc.append(p)\n",
    "    for p in proc:\n",
    "        p.join()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    run_in_parallel(scraping_split_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_in_parallel(scraping_split_one, scraping_split_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from 0 to 9\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "[Errno 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-f9f12742a530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScrapeATPStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-f265c79d10e9>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Scraping data from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" to \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgo_on_atp_ranking_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgo_on_player_profile_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgo_on_player_statistics_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-f265c79d10e9>\u001b[0m in \u001b[0;36mgo_on_atp_ranking_page\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# ---Going on the ATP Ranking page---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mmaximize_window\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw3c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW3C_MAXIMIZE_WINDOW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"windowHandle\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"current\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTemplate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstitute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.pyc\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsed_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhttplib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self, buffering)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1136\u001b[1;33m             \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1137\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jbadiabo\\Anaconda3\\envs\\dato-env\\lib\\socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: [Errno 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "x = ScrapeATPStats(range(10))\n",
    "x()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ten = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ten[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "until 9\n"
     ]
    }
   ],
   "source": [
    "print \"until_\" + str(ten[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_datetime_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_datetime.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
