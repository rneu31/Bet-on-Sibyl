{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import csv\n",
    "import sqlite3 as lite\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import *\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "from sklearn import linear_model, metrics, svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "\n",
    "# -------------------------------------------CLASS 'MAKE_PREDICTIONS'----------------------------------------------\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix'):\n",
    "    plt.title(title)\n",
    "    fig = plt.figure()\n",
    "    # create a grid of subplots\n",
    "    ax = fig.add_subplot(111)\n",
    "    # declare a var that shows the confusion matrix\n",
    "    cax = ax.matshow(cm)\n",
    "    # Create a colorbar for a ScalarMappable instance, mappable\n",
    "    fig.colorbar(cax)\n",
    "    plt.ylabel('True value')\n",
    "    plt.xlabel('Predicted value')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # ex of use : x = ATPMakePredictions(2016, \"atp_features_2000_2015.npz\", \"atp_team_data_2016.db\")\n",
    "\n",
    "\n",
    "def plot_response(subset_sizes, train_errs, test_errs):\n",
    "    plt.plot(subset_sizes, train_errs, lw=2)\n",
    "    plt.plot(subset_sizes, test_errs, lw=2)\n",
    "    plt.legend(['Training accuracy', 'Test accuracy'])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Dataset size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model response to dataset size')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normalize_features(X):\n",
    "    minmax_scale = preprocessing.MinMaxScaler().fit(X)\n",
    "    X = minmax_scale.transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def standardize_features(X):\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_id_column_to_csv(tableau_output_filename):\n",
    "    df = read_csv(tableau_output_filename)\n",
    "    df.index.name = \"ID\"\n",
    "    df.to_csv(tableau_output_filename, mode='w+', index=True)\n",
    "\n",
    "\n",
    "class ATPMakePredictions(object):\n",
    "    # Apply ML techniques to the ATP scrapped data and use of the resulting model to make predictions on unplayed games\n",
    "    # NB: unplayed games are in the atp_db_name_FOR_THE_CURRENT_SEASON (ex 'atp_db_name_2016')\n",
    "    # => season on which you want to\n",
    "    # make predictions\n",
    "\n",
    "    from ScrapeATPStats import ScrapeATPStats\n",
    "    from PrepareForMLTechATP import PrepareForML\n",
    "\n",
    "    def __init__(self, current_season, ps_feature_filename, atp_db_name):\n",
    "        self.data = np.load(ps_feature_filename)\n",
    "        self.current_season = current_season\n",
    "        self.tableau_output_filename = \"atp_tableau_output_\" + str(current_season) + '.csv'\n",
    "        self.X = self.data['X']\n",
    "        self.y = self.data['y']\n",
    "        self.atp_db_name = atp_db_name\n",
    "\n",
    "    def __call__(self):\n",
    "        print \"Scraping ATP current season data for update...\\n\"\n",
    "        self.acquire_current_season_data(self.current_season)\n",
    "        # Train once again our data from 'atp_features.npz' so as to show the learning curves\n",
    "        # Perform the training process in the CV function\n",
    "        print \"Algorithm training...\"\n",
    "        self.train_logistic_regression()\n",
    "        print \"OK\\n\"\n",
    "\n",
    "        print \"Making predictions...\"\n",
    "        self.make_tableau_file(self.game_data_filename, self.datetime_filename)\n",
    "        print \"OK\\n\"\n",
    "        add_id_column_to_csv(self.tableau_output_filename)\n",
    "\n",
    "    # ----------------------------ACQUIRE CURRENT SEASON DATA ------------------------------------------\n",
    "\n",
    "    def acquire_current_season_data(self, current_season):\n",
    "        # Acquires all data structures needed to make predictions on current season matchups\n",
    "\n",
    "        team_data_filename = 'atp_team_stats_' + str(current_season) + '_' + str(current_season) + '.csv'\n",
    "        game_data_filename = 'atp_game_stats_' + str(current_season) + '_' + str(current_season) + '.csv'\n",
    "        datetime_filename = 'date_atp_game_stats_' + str(current_season) + '_' + str(current_season) + '.csv'\n",
    "        atp_db_filename = 'atp_team_data_' + str(current_season) + '.db'\n",
    "        cs_feature_filename = 'atp_features_' + str(current_season) + '.npz'\n",
    "        # If you want to filter,\n",
    "        # uncomment and make changes to the corresponding section in the 'AcquireGameStats' class\n",
    "\n",
    "        # Scrape atp data for the current season\n",
    "        print \"Scraping Team Stats...\"\n",
    "        atp_team_data = self.AcquireATPTeamStats(current_season, current_season)\n",
    "        atp_team_data()\n",
    "        print \"OK\\n\"\n",
    "\n",
    "        print \"Scraping Game Stats...\"\n",
    "        atp_game_data = self.AcquireATPGameStats(current_season, current_season)\n",
    "        atp_game_data()\n",
    "        print \"OK\\n\"\n",
    "\n",
    "        print \"Preprocessing updated data for Machine Learning...\"\n",
    "        # Prepare for ML predictions \n",
    "        pml = self.PrepareForML(game_data_filename, atp_db_filename, team_data_filename)\n",
    "        pml(cs_feature_filename)\n",
    "        print \"OK\\n\"\n",
    "\n",
    "        self.datetime_filename = datetime_filename\n",
    "        self.game_data_filename = game_data_filename\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------\n",
    "    # ----------------------------------------CREATE CSV OUTPUT FOR TABLEAU -----------------------------\n",
    "\n",
    "    def make_tableau_file(self, game_data_filename, datetime_filename):\n",
    "        # Produces a csv file containing predicted\n",
    "        # and actual game results for the current season that can see in Tableau\n",
    "\n",
    "        with open(self.tableau_output_filename, 'wb') as writefile:\n",
    "            tableau_write = csv.writer(writefile)\n",
    "            tableau_write.writerow(\n",
    "                ['Visitor_Team', 'V_Team_PTS', 'Home_Team', 'H_Team_PTS', 'True_Result', 'Predicted_Result',\n",
    "                 'Confidence', 'Date'])\n",
    "\n",
    "            with open(game_data_filename, 'rb') as readfile, open(datetime_filename, 'rb') as readfile2:\n",
    "                scorereader = csv.reader(readfile)\n",
    "                scores = [row for row in scorereader]\n",
    "                scores = scores[1::]\n",
    "                daysreader = csv.reader(readfile2)\n",
    "                days = [day for day in daysreader]\n",
    "                if (len(scores) != len(days)):\n",
    "                    print(\"File lengths do not match\")\n",
    "                else:\n",
    "                    for i in range(len(days)):\n",
    "                        tableau_content = scores[i][1::]\n",
    "                        tableau_date = days[i]\n",
    "                        # Append True_Result\n",
    "                        try:\n",
    "                            if int(tableau_content[3]) > int(tableau_content[1]):\n",
    "                                tableau_content.append(1)\n",
    "                            else:\n",
    "                                tableau_content.append(0)\n",
    "                        except:\n",
    "                            pass\n",
    "                        # Append Predicted_Result and Confidence\n",
    "                        prediction_results = self.make_predictions(tableau_content[0], tableau_content[2])\n",
    "                        tableau_content += list(prediction_results)\n",
    "                        tableau_content += tableau_date\n",
    "\n",
    "                        tableau_write.writerow(tableau_content)\n",
    "\n",
    "                        # ------------------------- ADD INDEX COLUMN TO CSV FOR TABLEAU ANALYSIS ---------\n",
    "\n",
    "                        # NB: To run AFTER the 'make_tableau_file'\n",
    "                        # function so as to add an 'ID' column to the tableau_output file and thus indexing\n",
    "                        # each game. => Ease the analysis of each game in Tableau\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------\n",
    "    # ----------------------------------------------ALGORITHMS-----------------------------------------------------------\n",
    "\n",
    "    # NB!!!! Each of the algorithms has\n",
    "    # a separation between 'intanciating + training to the data' and only 'instanciating'\n",
    "    # which required for k-fold CV\n",
    "\n",
    "    # --------Logistic Regression----------\n",
    "    def train_logistic_regression(self, scale_data=False):\n",
    "        # IF you pass 'scale_data' = True,\n",
    "        # DO NOT FORGET to do the same in 'make_predictions' function to also normalize test\n",
    "        # test data (current season).\n",
    "        # Preprocessing step: False by default. Scaling the feature vector applying normalization\n",
    "        # 'Min-Max scaling' technique. If you want to use standardization use 'standardize_features' function instead\n",
    "        # Hint: Better use normalization for SVM \n",
    "\n",
    "        if scale_data != False:\n",
    "            self.X = standardize_features(self.X)\n",
    "        else:\n",
    "            pass\n",
    "        X, y = shuffle(self.X, self.y)\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "        self.logreg.fit(X, y)\n",
    "\n",
    "    def instantiate_logistic_regression(self):\n",
    "        # Only instantiate a linear model without fitting any data\n",
    "        # Needed in the model evaluation function\n",
    "        self.logreg2 = linear_model.LogisticRegression()\n",
    "        pass\n",
    "\n",
    "    # ---------Radial Basis Function kernel SVM------------\n",
    "    def train_rbf_svm(self, scale_data=False):\n",
    "        # Same hint => See hint for 'train_logistic_regression'\n",
    "\n",
    "        if scale_data != False:\n",
    "            self.X = standardize_features(self.X)\n",
    "        else:\n",
    "            pass\n",
    "        X, y = shuffle(self.X, self.y)\n",
    "        self.clf = svm.SVC(probability=True, random_state=None)\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def instantiate_rbf_svm(self):\n",
    "        self.clf2 = svm.SVC(probability=True, random_state=None)\n",
    "        pass\n",
    "\n",
    "    # --------Ensemble Methods: AdaBoostClassifier------------\n",
    "\n",
    "    def train_adaboost(self):\n",
    "        X, y = shuffle(self.X, self.y)\n",
    "        self.dbt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
    "        self.dbt.fit(X, y)\n",
    "\n",
    "    def instantiate_adaboost(self):\n",
    "        self.dbt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
    "        pass\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "    # ------------------------MAKE_PREDICTION Function (implemented in 'make_tableau_file' function------\n",
    "\n",
    "    def make_predictions(self, team1, team2, scale_data=False):\n",
    "        # Using the chosen classifier,\n",
    "        # return 1 if the the model thinks Home_Team (team2)will beat the Visitor_Team (team1)\n",
    "        # NB: Respect the order for the implementation in the 'make_tableau_file' function. Visitor_team = team1 \n",
    "        # Home_team = team2 => for consistency with the PrepareForML class techniques\n",
    "\n",
    "        query = 'SELECT * FROM Team_Stats WHERE Team = ?'\n",
    "\n",
    "        con = lite.connect(self.atp_db_name)\n",
    "        with con:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(query, (team1,))\n",
    "            feature1 = list(cur.fetchone()[2::])\n",
    "            cur.execute(query, (team2,))\n",
    "            feature2 = list(cur.fetchone()[2::])\n",
    "            feature = np.array(feature2).reshape(1, -1) - np.array(feature1).reshape(1, -1)\n",
    "\n",
    "            if scale_data != False:\n",
    "                feature = normalize_features(feature)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Make prediction part: to change according the classifier you want to use\n",
    "            # available classifiers: logreg, clf, dbt (change 2X when you choose a classifier)\n",
    "            prediction_output = self.logreg.predict(feature)  # Predict class labels for samples in X\n",
    "            prediction_probability = max(\n",
    "                self.logreg.predict_proba(feature)[0])  # Returns the probabily of the prediction\n",
    "\n",
    "            return prediction_output[0], prediction_probability  # Why 'prediction_output[0]... Cannot remember...\n",
    "\n",
    "            # ---------------------------------------PARAMETRIC------------------------------------------------------\n",
    "\n",
    "    def cval_score(self):\n",
    "        # change 'logreg' to 'clf' if you want to perform it on SVM (dbt also available)\n",
    "        scores = cross_val_score(self.logreg, self.X, self.y, cv=10)\n",
    "        print scores.mean(), scores.std()\n",
    "\n",
    "    # Used in for learning curves and model evaluation\n",
    "    def train_test_split(self):\n",
    "        self.trX, self.teX, self.trY, self.teY = train_test_split(self.X, self.y, test_size=0.30, random_state=None)\n",
    "        pass\n",
    "\n",
    "    # ----------------------------------------CROSS VALIDATION AND MODEL EVALUATION ------------------------\n",
    "\n",
    "    # The below function takes a model and make a evaluation:\n",
    "    # a pre-split dataset(train/test X and y array (nb: use train_test_split\n",
    "    # function to do so)),\n",
    "    # a scoring function as input and iterates through the dataset training on n exponentially spaced subsets and\n",
    "    # returns the learning curves e.g. score_func = metrics.accuracy_score\n",
    "\n",
    "    def data_size_response(self, model, score_func, prob=True, n_subsets=10):\n",
    "        # creating 2 empty arrays for train\n",
    "        train_errs, test_errs = [], []\n",
    "        # defining a var that take the value of n_subsets and creates n_subsets with corresponding size.\n",
    "        # \"linspace returns num evenly spaced samples,\n",
    "        # calculated over the interval [start, stop]. shape[0] of an array y of dimension (n,p) merely means\n",
    "        # 'n', => n of rows\n",
    "        subset_sizes = np.exp(np.linspace(3, np.log(self.trX.shape[0]), n_subsets)).astype(int)\n",
    "\n",
    "        # looping over the subset sizes \n",
    "        for m in subset_sizes:\n",
    "\n",
    "            #  for each subset we fit the model on trX, trY\n",
    "            model.fit(self.trX[:m], self.trY[:m])\n",
    "\n",
    "            # if prob is 'True',\n",
    "            # we define var'train_err' & 'test_err' which are scores of Y (true Y) and 'predict_proba' made on X\n",
    "            # (predict Y). respectively for the train and test set\n",
    "            if prob:\n",
    "                train_err = score_func(self.trY[:m], model.predict_proba(self.trX[:m]))  # m is the size of the subset\n",
    "                test_err = score_func(self.teY, model.predict_proba(self.teX))\n",
    "\n",
    "            else:\n",
    "                train_err = score_func(self.trY[:m], model.predict(self.trX[:m]))\n",
    "                test_err = score_func(self.teY, model.predict(self.teX))\n",
    "\n",
    "            # printing the results for the m subset\n",
    "            print \"training error(accuracy): %.3f test error(accuracy): %.3f subset size: %.3f\" % (\n",
    "                train_err, test_err, m)\n",
    "\n",
    "            # appending the m results to the arrays 'train_errs' & 'test_errs'\n",
    "            train_errs.append(train_err)\n",
    "            test_errs.append(test_err)\n",
    "\n",
    "        # returning the number of subsets and the arrays\n",
    "        return subset_sizes, train_errs, test_errs\n",
    "\n",
    "    # Plotting function for visualizing the above response, e.g. the train error and test error\n",
    "\n",
    "    # Instruction to execute the above functions\n",
    "    # model = it is the model we use e.g LogisticRegression()\n",
    "    # score_func = it is the score function we use e.g. cval_score()\n",
    "    # We declare a variable response = data_size_response(model, trX, teX, trY, teY, score_func, prob=True, n_subsets=?)\n",
    "    # We plot the response: plot_response(*response)\n",
    "\n",
    "    def model_evaluation(self):\n",
    "\n",
    "        # Do not forget to replace the classifier by the one you want to evaluate\n",
    "        # Classifiers: logreg2, clf2, dbt2\n",
    "        self.train_test_split()\n",
    "        y_pred = self.logreg2.fit(self.trX, self.trY).predict(self.teX)\n",
    "        # predictions are in columns and actual values in rows\n",
    "        cm = metrics.confusion_matrix(self.teY, y_pred)\n",
    "        # nb: The support is the number of occurrences of each class in y_true (e.g. what really happened)\n",
    "        print metrics.classification_report(self.teY, y_pred)\n",
    "        print \"matthews correlation coefficent: %.2f\" % (metrics.matthews_corrcoef(self.teY, y_pred))\n",
    "        accuracy = float(cm[0][0] + cm[1][1]) / cm.sum()\n",
    "        print \"accuracy: %.2f\" % (accuracy)\n",
    "        away_accuracy = float(cm[0][0]) / (cm[0][0] + cm[0][1])\n",
    "        print \"away_accuracy: %.2f\" % (away_accuracy)\n",
    "        home_accuracy = float(cm[1][1]) / (cm[1][1] + cm[1][0])\n",
    "        print \"home_accuracy: %.2f\" % (home_accuracy)\n",
    "        plot_confusion_matrix(cm, title='Confusion matrix')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
